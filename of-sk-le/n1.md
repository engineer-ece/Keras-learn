Here’s a **step-by-step mathematical, theoretical, and code breakdown** for the requested machine learning algorithms as per your notes:

---

### 1. **Linear Regression**
**Theory**: 
- Models the relationship between a dependent variable $y$ and one or more independent variables $x$.
- The linear equation is $ y = w_0 + w_1X + \epsilon $, where $ w_0 $ is the intercept, $ w_1 $ is the slope, and $ \epsilon $ is the error term.

**Mathematical Steps**:
1. Compute predictions $ \hat{y} = w_0 + w_1 X $.
2. Use **cost function** (MSE): 
   $$
   J(w_0, w_1) = \frac{1}{2n} \sum_{i=1}^n (\hat{y}_i - y_i)^2
   $$
3. Minimize the cost function using **gradient descent**:
   $$
   w_j := w_j - \alpha \cdot \frac{\partial J(w)}{\partial w_j}
   $$

**Python Code**:
```python
from sklearn.linear_model import LinearRegression
X = [[1300], [1500], [1700], [1830]]  # Features (Square Feet)
y = [240, 320, 330, 295]  # Target (House Prices)

model = LinearRegression()
model.fit(X, y)
pred = model.predict([[1600]])
print(f"Predicted House Price: {pred}")
```

---

### 2. **Polynomial Regression**
**Theory**:
- Extends linear regression by fitting a polynomial relationship between the dependent and independent variables: $ y = w_0 + w_1 X + w_2 X^2 + ... + w_n X^n $.

**Mathematical Steps**:
1. Transform the features into polynomial features.
2. Apply linear regression to the transformed features.

**Python Code**:
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

X = np.array([[1300], [1500], [1700], [1830]])
y = [240, 320, 330, 295]

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression()
model.fit(X_poly, y)

pred = model.predict(poly.transform([[1600]]))
print(f"Predicted House Price (Poly): {pred}")
```

---

### 3. **Logistic Regression**
**Theory**:
- Logistic regression is used for binary classification, where the output is the probability of a class. The equation is:
  $$
  P(y=1|x) = \frac{1}{1 + e^{-(w_0 + w_1X)}}
  $$

**Mathematical Steps**:
1. Use the **sigmoid function** to predict probabilities:
   $$
   h(x) = \frac{1}{1 + e^{-z}}, \quad z = \theta^T x
   $$
2. Minimize the **log-loss** cost function.

**Python Code**:
```python
from sklearn.linear_model import LogisticRegression
X = [[0], [1], [2], [3], [4]]  # Example feature data
y = [0, 0, 1, 1, 1]  # Binary target

model = LogisticRegression()
model.fit(X, y)
print(model.predict([[2.5]]))  # Prediction: Class label
```

---

### 4. **Decision Trees**
**Theory**:
- Splits data into subsets based on feature values to form a tree. Each split is determined to minimize **Gini Impurity** or **Entropy**.

**Mathematical Steps**:
1. Calculate Gini Impurity for each split:
   $$
   Gini = 1 - \sum p_i^2
   $$
2. Choose the split that minimizes the weighted Gini Impurity.

**Python Code**:
```python
from sklearn.tree import DecisionTreeClassifier
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features
y = [1, 1, 0, 0]  # Target

clf = DecisionTreeClassifier()
clf.fit(X, y)
print(clf.predict([[1, 0]]))  # Predict class
```

---

### 5. **Random Forest**
**Theory**:
- An ensemble method using multiple decision trees where each tree is built on a subset of the data and features.

**Mathematical Steps**:
1. Build multiple decision trees.
2. Aggregate the results (majority voting for classification, averaging for regression).

**Python Code**:
```python
from sklearn.ensemble import RandomForestClassifier
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features
y = [1, 1, 0, 0]  # Target

model = RandomForestClassifier(n_estimators=10)
model.fit(X, y)
print(model.predict([[1, 1]]))
```

---

### 6. **K-Nearest Neighbors (KNN)**
**Theory**:
- KNN is a non-parametric method used for classification or regression by finding the nearest $k$ neighbors.

**Mathematical Steps**:
1. Calculate the Euclidean distance:
   $$
   d(x_i, x_j) = \sqrt{\sum (x_i - x_j)^2}
   $$
2. Find the $k$ nearest neighbors and use their labels to predict.

**Python Code**:
```python
from sklearn.neighbors import KNeighborsClassifier
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features
y = [1, 1, 0, 0]  # Target

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X, y)
print(model.predict([[0.5, 0.5]]))
```

---

### 7. **Naive Bayes**
**Theory**:
- Naive Bayes is based on **Bayes’ Theorem** with the assumption that features are conditionally independent:
  $$
  P(y|x_1, x_2, ..., x_n) \propto P(y) \prod P(x_i|y)
  $$

**Mathematical Steps**:
1. Calculate prior probabilities $ P(y) $.
2. Calculate likelihoods $ P(x_i|y) $.

**Python Code**:
```python
from sklearn.naive_bayes import GaussianNB
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features
y = [1, 1, 0, 0]  # Target

model = GaussianNB()
model.fit(X, y)
print(model.predict([[0, 1]]))
```

---

### 8. **Support Vector Machine (SVM)**
**Theory**:
- SVM finds the hyperplane that best separates the data into classes by maximizing the **margin** between the closest points (support vectors).

**Mathematical Steps**:
1. Find the optimal hyperplane by solving:
   $$
   \min_w \frac{1}{2} ||w||^2
   $$
   subject to:
   $$
   y_i(w^T x_i + b) \geq 1
   $$

**Python Code**:
```python
from sklearn.svm import SVC
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features
y = [1, 1, 0, 0]  # Target

model = SVC(kernel='linear')
model.fit(X, y)
print(model.predict([[1, 1]]))
```

---

### 9. **K-Means Clustering**
**Theory**:
- Clustering algorithm that partitions data into $k$ clusters, minimizing the within-cluster sum of squares.

**Mathematical Steps**:
1. Assign each point to the nearest centroid.
2. Recalculate the centroids.
3. Repeat until convergence.

**Python Code**:
```python
from sklearn.cluster import KMeans
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features

model = KMeans(n_clusters=2)
model.fit(X)
print(model.labels_)  # Cluster labels
```

---

### 10. **Hierarchical Clustering**
**Theory**:
- Clustering method that builds a hierarchy of clusters using either **agglomerative** (bottom-up) or **divisive** (top-down) approaches.

**Mathematical Steps**:
1. Start with each data point as its own cluster.
2. Merge the closest clusters iteratively.

**Python Code**:
```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
X = [[1, 1], [1, 0], [0, 1], [0, 0]]  # Features

linked = linkage(X, 'single')
dendrogram(linked)
plt.show()
```

---

### Summary of Use Cases:
- **Linear Regression**: Predicting continuous values (e.g., house prices).
- **Polynomial Regression**: For modeling curvilinear relationships.
- **Logistic Regression

**: Binary classification (e.g., spam detection).
- **Decision Trees**: Both classification and regression tasks.
- **Random Forest**: Ensemble method for improved accuracy.
- **KNN**: Simple classification and regression based on proximity.
- **Naive Bayes**: Text classification and spam filtering.
- **SVM**: High-dimensional classification tasks (e.g., image recognition).
- **K-Means**: Grouping unlabeled data into clusters.
- **Hierarchical Clustering**: When a hierarchy of clusters is needed.